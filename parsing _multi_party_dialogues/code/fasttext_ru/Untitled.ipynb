{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==1.15.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/d3/4501a075355733b90bc384c900ae4f84df702c340f4034741647c63a5acd/tensorflow-1.15.3-cp37-cp37m-manylinux2010_x86_64.whl (110.5MB)\n",
      "\u001b[K     |████████████████████████████████| 110.5MB 7.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astor>=0.6.0 (from tensorflow==1.15.3)\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/astor/\u001b[0m\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\n",
      "Collecting keras-preprocessing>=1.0.5 (from tensorflow==1.15.3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 5.2MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /home/vvs/anaconda3/lib/python3.7/site-packages (from tensorflow==1.15.3) (1.17.2)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==1.15.3)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting protobuf>=3.6.1 (from tensorflow==1.15.3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/dc/5ba56eab7440c62c5f808b4267e2a1d6c136e90293b43fefb1b493c6d704/protobuf-3.13.0-cp37-cp37m-manylinux1_x86_64.whl (1.3MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3MB 4.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator==1.15.1 (from tensorflow==1.15.3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
      "\u001b[K     |████████████████████████████████| 512kB 2.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2 (from tensorflow==1.15.3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl (65kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 1.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.7.0 (from tensorflow==1.15.3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/07/f69dd3367368ad69f174bfe426a973651412ec11d48ec05c000f19fe0561/absl_py-0.10.0-py3-none-any.whl (127kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 4.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-applications>=1.0.8 (from tensorflow==1.15.3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 2.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.6 (from tensorflow==1.15.3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl (57kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 3.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /home/vvs/anaconda3/lib/python3.7/site-packages (from tensorflow==1.15.3) (1.12.0)\n",
      "Collecting tensorboard<1.16.0,>=1.15.0 (from tensorflow==1.15.3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8MB 841kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.8.6 (from tensorflow==1.15.3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/6f/6d0bd5ace7d87a57b7b02608fab1317d8e585b6a827a59572ed261ca8833/grpcio-1.31.0-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3MB 1.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /home/vvs/anaconda3/lib/python3.7/site-packages (from tensorflow==1.15.3) (1.11.2)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/vvs/anaconda3/lib/python3.7/site-packages (from tensorflow==1.15.3) (0.33.6)\n",
      "Collecting gast==0.2.2 (from tensorflow==1.15.3)\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Requirement already satisfied: setuptools in /home/vvs/anaconda3/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow==1.15.3) (41.4.0)\n",
      "Requirement already satisfied: h5py in /home/vvs/anaconda3/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==1.15.3) (2.9.0)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.3)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/63/eaec2bd025ab48c754b55e8819af0f6a69e2b1e187611dd40cbbe101ee7f/Markdown-3.2.2-py3-none-any.whl (88kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 6.5MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /home/vvs/anaconda3/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.3) (0.16.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/vvs/anaconda3/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.3) (0.23)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/vvs/anaconda3/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.3) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /home/vvs/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.3) (7.2.0)\n",
      "Building wheels for collected packages: termcolor, gast\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-cp37-none-any.whl size=4832 sha256=04a0e213075ed7e10371a75452585965bc128ec3501eb3027fff04c16d89bca6\n",
      "  Stored in directory: /home/vvs/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=b3b3441060a3387f5608c516a4f6bc7bcdbf82b5f61c37fc73826422f8030497\n",
      "  Stored in directory: /home/vvs/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "Successfully built termcolor gast\n",
      "Installing collected packages: astor, keras-preprocessing, termcolor, protobuf, tensorflow-estimator, opt-einsum, absl-py, keras-applications, google-pasta, grpcio, markdown, tensorboard, gast, tensorflow\n",
      "Successfully installed absl-py-0.10.0 astor-0.8.1 gast-0.2.2 google-pasta-0.2.0 grpcio-1.31.0 keras-applications-1.0.8 keras-preprocessing-1.1.2 markdown-3.2.2 opt-einsum-3.3.0 protobuf-3.13.0 tensorboard-1.15.0 tensorflow-1.15.3 tensorflow-estimator-1.15.1 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==1.15.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.3'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "DuplicateFlagError",
     "evalue": "The flag 'display_interval' is defined twice. First from /home/vvs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py, Second from /home/vvs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py.  Description from first occurrence: step interval to display information",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDuplicateFlagError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d56972bd95e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display_interval'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'step interval to display information'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'show_predictions'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'show predictions in the test stage'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'word_vector'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/home/vvs/Downloads/d_parse/models/ft_freqprune_100K_20K_pq_100.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'word vector'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/platform/flags.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m           \u001b[0;34m'Use of the keyword argument names (flag_name, default_value, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m           'docstring) is deprecated, please use (name, default, help) instead.')\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_integer\u001b[0;34m(name, default, help, lower_bound, upper_bound, flag_values, **args)\u001b[0m\n\u001b[1;32m    366\u001b[0m   \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegerParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlower_bound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper_bound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m   \u001b[0mserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEFINE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m   \u001b[0m_register_bounds_validator_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflag_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE\u001b[0;34m(parser, name, default, help, flag_values, serializer, module_name, **args)\u001b[0m\n\u001b[1;32m    101\u001b[0m   return DEFINE_flag(\n\u001b[1;32m    102\u001b[0m       \u001b[0m_flag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m       module_name)\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_flag\u001b[0;34m(flag, flag_values, module_name)\u001b[0m\n\u001b[1;32m    126\u001b[0m   \u001b[0;31m# Copying the reference to flag_values prevents pychecker warnings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m   \u001b[0mfv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m   \u001b[0mfv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m   \u001b[0;31m# Tell flag_values who's defining the flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, name, flag)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;31m# module is simply being imported a subsequent time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDuplicateFlagError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_flag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m     \u001b[0mshort_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshort_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0;31m# If a new flag overrides an old one, we need to cleanup the old flag's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDuplicateFlagError\u001b[0m: The flag 'display_interval' is defined twice. First from /home/vvs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py, Second from /home/vvs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py.  Description from first occurrence: step interval to display information"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os, random, time\n",
    "import compress_fasttext\n",
    "from Model import Model\n",
    "from utils import load_data, build_vocab, preview_data\n",
    "\n",
    "if \"CUDA_VISIBLE_DEVICES\" not in os.environ: \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "tf.flags.DEFINE_boolean('train1', True, 'train model')\n",
    "tf.flags.DEFINE_integer('display_interval', 500, 'step interval to display information')\n",
    "tf.flags.DEFINE_boolean('show_predictions', False, 'show predictions in the test stage')\n",
    "tf.flags.DEFINE_string('word_vector', '/home/vvs/Downloads/d_parse/models/ft_freqprune_100K_20K_pq_100.bin', 'word vector')\n",
    "tf.flags.DEFINE_string('prefix', 'dev', 'prefix for storing model and log')\n",
    "tf.flags.DEFINE_integer('vocab_size', 1000, 'vocabulary size')\n",
    "tf.flags.DEFINE_integer('max_edu_dist', 20, 'maximum distance between two related edus') \n",
    "tf.flags.DEFINE_integer('dim_embed_word', 300, 'dimension of word embedding')\n",
    "tf.flags.DEFINE_integer('dim_embed_relation', 300, 'dimension of relation embedding')\n",
    "tf.flags.DEFINE_integer('dim_feature_bi', 4, 'dimension of binary features')\n",
    "tf.flags.DEFINE_boolean('use_structured', True, 'use structured encoder')\n",
    "tf.flags.DEFINE_boolean('use_speaker_attn', True, 'use speaker highlighting mechanism')\n",
    "tf.flags.DEFINE_boolean('use_shared_encoders', False, 'use shared encoders')\n",
    "tf.flags.DEFINE_boolean('use_random_structured', False, 'use random structured repr.')\n",
    "tf.flags.DEFINE_integer('num_epochs', 50, 'number of epochs')\n",
    "tf.flags.DEFINE_integer('num_units', 256, 'number of hidden units')\n",
    "tf.flags.DEFINE_integer('num_layers', 1, 'number of RNN layers in encoders')\n",
    "tf.flags.DEFINE_integer('num_relations', 16, 'number of relation types')\n",
    "tf.flags.DEFINE_integer('batch_size', 4, 'batch size')\n",
    "tf.flags.DEFINE_float('keep_prob', 0.5, 'probability to keep units in dropout')\n",
    "tf.flags.DEFINE_float('learning_rate', 0.1, 'learning rate')\n",
    "tf.flags.DEFINE_float('learning_rate_decay', 0.98, 'learning rate decay factor')\n",
    "    \n",
    "def get_summary_sum(s, length):\n",
    "    loss_bi, loss_multi = s[0] / length, s[1] / length\n",
    "    prec_bi, recall_bi = s[4] * 1. / s[3], s[4] * 1. / s[2]\n",
    "    f1_bi = 2 * prec_bi * recall_bi / (prec_bi + recall_bi)\n",
    "    prec_multi, recall_multi = s[5] * 1. / s[3], s[5] * 1. / s[2]\n",
    "    f1_multi = 2 * prec_multi * recall_multi / (prec_multi + recall_multi)\n",
    "    return [loss_bi, loss_multi, f1_bi, f1_multi]    \n",
    "    \n",
    "map_relations = {}\n",
    "data_train = load_data('/home/vvs/Downloads/d_parse/chinadataset/train_spect_ru.json', map_relations)\n",
    "data_test = load_data('/home/vvs/Downloads/d_parse/chinadataset/TEST_spect_ru.json', map_relations)\n",
    "valid_size = int(FLAGS.valid_ratio * len(data_train))\n",
    "data_valid = data_train[-valid_size:]\n",
    "data_train = data_train[:-valid_size]\n",
    "vocab, embed = build_vocab(data_train)\n",
    "print(\"Dataset sizes: %d/%d/%d\" % (len(data_train), len(data_test), len(data_valid)))\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "with sess.as_default():\n",
    "    model = Model(sess, FLAGS, embed, data_train)\n",
    "\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    global_step_inc_op = global_step.assign(global_step + 1)    \n",
    "    epoch = tf.Variable(0, name=\"epoch\", trainable=False)\n",
    "    epoch_inc_op = epoch.assign(epoch + 1)\n",
    "\n",
    "    saver = tf.train.Saver(\n",
    "        write_version=tf.train.SaverDef.V2,\n",
    "        max_to_keep=None, \n",
    "        pad_step_number=True, \n",
    "        keep_checkpoint_every_n_hours=1.0\n",
    "    )        \n",
    "    \n",
    "    summary_list = [\n",
    "        \"loss_bi\", \"loss_multi\",\n",
    "        \"prec_bi\", \"recall_bi\", \"f1_bi\",\n",
    "        \"prec_multi\", \"recall_multi\", \"f1_multi\"\n",
    "    ]\n",
    "    summary_num = len(summary_list)\n",
    "    len_output_feed = 6\n",
    "\n",
    "    if FLAGS.is_train:\n",
    "        if tf.train.get_checkpoint_state(FLAGS.model_dir):\n",
    "            print(\"Reading model parameters from %s\" % FLAGS.model_dir)\n",
    "            saver.restore(sess, tf.train.latest_checkpoint(FLAGS.model_dir))\n",
    "        else:\n",
    "            print(\"Created model with fresh parameters\")\n",
    "            sess.run(tf.global_variables_initializer())  \n",
    "            model.initialize(vocab)          \n",
    "            \n",
    "        print(\"Trainable variables:\")\n",
    "        for var in tf.trainable_variables():\n",
    "            print(var)\n",
    "\n",
    "        train_writer = tf.summary.FileWriter(FLAGS.log_dir+'/'+ \"train\")\n",
    "        valid_writer = tf.summary.FileWriter(FLAGS.log_dir+'/'+\"valid\")\n",
    "        test_writer = tf.summary.FileWriter(FLAGS.log_dir+'/'+\"test\")\n",
    "        summary_placeholders = [tf.placeholder(tf.float32) for i in range(summary_num)]\n",
    "        summary_op = [tf.summary.scalar(summary_list[i], summary_placeholders[i]) for i in range(summary_num)]\n",
    "        \n",
    "        train_batches = get_batches(data_train, FLAGS.batch_size)\n",
    "        valid_batches = get_batches(data_valid, FLAGS.batch_size)\n",
    "        test_batches = get_batches(data_test, FLAGS.batch_size)\n",
    "        \n",
    "        best_test_f1 = [0] * 2\n",
    "        while True:\n",
    "            epoch_inc_op.eval()\n",
    "            summary_steps = 0\n",
    "            \n",
    "            random.shuffle(train_batches)\n",
    "            start_time = time.time()\n",
    "            s = np.zeros(len_output_feed)\n",
    "            \n",
    "            for batch in train_batches:\n",
    "                ops = model.step(batch, is_train=True)\n",
    "                    \n",
    "                for i in range(len_output_feed):\n",
    "                    s[i] += ops[i]\n",
    "                        \n",
    "                summary_steps += 1\n",
    "                global_step_inc_op.eval()\n",
    "                global_step_val = global_step.eval()         \n",
    "                if global_step_val % FLAGS.display_interval == 0:\n",
    "                    print(\"epoch %d, global step %d (%.4fs/step):\" % (\n",
    "                        epoch.eval(), global_step_val, \n",
    "                        (time.time() - start_time) * 1. / summary_steps\n",
    "                    ))\n",
    "                    summary_sum = get_summary_sum(s, summary_steps)\n",
    "                    for k in range(summary_num):\n",
    "                        print(\"  train %s: %.5lf\" % (\n",
    "                            summary_list[k], \n",
    "                            summary_sum[k]\n",
    "                        ))\n",
    "                        \n",
    "                    print(\"  best test f1:\", best_test_f1[0], best_test_f1[1])\n",
    "                        \n",
    "            summary_sum = get_summary_sum(s, len(train_batches))            \n",
    "            summaries = sess.run(summary_op, feed_dict=dict(list(zip(summary_placeholders, summary_sum))))\n",
    "            for s in summaries:\n",
    "                train_writer.add_summary(summary=s, global_step=epoch.eval())                        \n",
    "            print(\"epoch %d (learning rate %.5lf)\" % \\\n",
    "                (epoch.eval(), model.learning_rate.eval()))\n",
    "            for k in range(summary_num):\n",
    "                print(\"  train %s: %.5lf\" % (summary_list[k], summary_sum[k]))                   \n",
    "            \n",
    "            s = np.zeros(len_output_feed)\n",
    "            for batch in valid_batches:\n",
    "                ops = model.step(batch)\n",
    "                for i in range(len_output_feed):\n",
    "                    s[i] += ops[i]\n",
    "            summary_sum = get_summary_sum(s, len(valid_batches))\n",
    "            summaries = sess.run(summary_op, feed_dict=dict(list(zip(summary_placeholders, summary_sum))))\n",
    "            for s in summaries:\n",
    "                valid_writer.add_summary(summary=s, global_step=epoch.eval())\n",
    "            for k in range(summary_num):\n",
    "                print(\"  valid %s: %.5lf\" % (summary_list[k], summary_sum[k]))                              \n",
    "            \n",
    "            s = np.zeros(len_output_feed)\n",
    "            random.seed(0)\n",
    "            for batch in test_batches:\n",
    "                ops = model.step(batch)\n",
    "                for i in range(len_output_feed):\n",
    "                    s[i] += ops[i]\n",
    "            summary_sum = get_summary_sum(s, len(test_batches))\n",
    "            summaries = sess.run(summary_op, feed_dict=dict(list(zip(summary_placeholders, summary_sum))))\n",
    "            for s in summaries:\n",
    "                test_writer.add_summary(summary=s, global_step=epoch.eval())            \n",
    "            for k in range(summary_num):\n",
    "                print(\"  test %s: %.5lf\" % (summary_list[k], summary_sum[k])) \n",
    "                \n",
    "            if summary_sum[-1] > best_test_f1[1]:\n",
    "                best_test_f1[0] = summary_sum[-4]\n",
    "                best_test_f1[1] = summary_sum[-1]\n",
    "            \n",
    "            print(\"  best test f1:\", best_test_f1[0], best_test_f1[1])\n",
    "            \n",
    "            model.learning_rate_decay_op.eval()             \n",
    "            \n",
    "            saver.save(sess, \"%s/checkpoint\" % FLAGS.model_dir, global_step=epoch.eval())                                            \n",
    "    else:\n",
    "        print(\"Reading model parameters from %s\" % FLAGS.model_dir) \n",
    "        saver.restore(sess, tf.train.latest_checkpoint(FLAGS.model_dir))\n",
    "        \n",
    "        test_batches = get_batches(data_test, 1, sort=False)\n",
    "    \n",
    "        s = np.zeros(len_output_feed)\n",
    "        random.seed(0)\n",
    "        idx = 0\n",
    "        cnt_golden, cnt_pred, cnt_cor_bi, cnt_cor_multi = [], [], [], []\n",
    "        for k, batch in enumerate(test_batches):\n",
    "            if len(batch[0][\"edus\"]) == 1: \n",
    "                continue\n",
    "            \n",
    "            ops = model.step(batch)\n",
    "            for i in range(len_output_feed):\n",
    "                s[i] += ops[i]\n",
    "                \n",
    "            if FLAGS.show_predictions:\n",
    "                idx = preview_data(batch, ops[-1], map_relations, vocab, idx)\n",
    "            \n",
    "            cnt_golden.append(ops[2])    \n",
    "            cnt_pred.append(ops[3])    \n",
    "            cnt_cor_bi.append(ops[4])    \n",
    "            cnt_cor_multi.append(ops[5])    \n",
    "                \n",
    "        summary_sum = get_summary_sum(s, len(test_batches))\n",
    "        \n",
    "        print(cnt_golden)\n",
    "        print(cnt_pred)\n",
    "        print(cnt_cor_bi)\n",
    "        print(cnt_cor_multi)\n",
    "        print(\"Test:\")\n",
    "        for k in range(summary_num):\n",
    "            print(\"  test %s: %.5lf\" % (summary_list[k], summary_sum[k]))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
