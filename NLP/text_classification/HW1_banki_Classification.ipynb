{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3. Классификация текстов [40/100]\n",
    "\n",
    "Сформулируем для простоты задачу бинарной классификации: будем классифицировать на два класса, то есть, различать резко отрицательные отзывы (с оценкой 1) и положительные отзывы (с оценкой 5). \n",
    "\n",
    "1.  Составьте обучающее и тестовое множество: выберите из всего набора данных N1 отзывов с оценкой 1 и N2 отзывов с оценкой 5 (значение N1 и N2 – на ваше усмотрение). Используйте ```sklearn.model_selection.train_test_split``` для разделения множества отобранных документов на обучающее и тестовое. \n",
    "2. Используйте любой известный вам алгоритм классификации текстов для решения задачи и получите baseline. Сравните разные варианты векторизации текста: использование только униграм, пар или троек слов или с использованием символьных $n$-грам. \n",
    "3. Сравните, как изменяется качество решения задачи при использовании скрытых тем в качестве признаков:\n",
    "* 1-ый вариант: $tf-idf$ преобразование (```sklearn.feature_extraction.text.TfidfTransformer```) и сингулярное разложение (оно же – латентый семантический анализ) (```sklearn.decomposition.TruncatedSVD```), \n",
    "* 2-ой вариант: тематические модели LDA (```sklearn.decomposition.LatentDirichletAllocation```). \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import bz2\n",
    "import regex\n",
    "from tqdm import tqdm\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "from pymystem3 import Mystem\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "from gensim.corpora import *\n",
    "from gensim.models import  *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from gensim.sklearn_api import LsiTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201030it [02:51, 1169.50it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "responses = []\n",
    "with bz2.BZ2File('banki_responses.json.bz2', 'r') as thefile:\n",
    "    for row in tqdm(thefile):\n",
    "        resp = json.loads(row)\n",
    "        if not resp['rating_not_checked'] and (len(resp['text'].split()) > 0):\n",
    "            responses.append(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Датасет\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = []\n",
    "for resp in responses:\n",
    "#     print(list(resp.values()))\n",
    "    train_list.append(list(resp.values()))\n",
    "train = pd.DataFrame(columns = list(responses[99].keys()), data = train_list)\n",
    "del train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>rating_not_checked</th>\n",
       "      <th>title</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>bank_license</th>\n",
       "      <th>author</th>\n",
       "      <th>bank_name</th>\n",
       "      <th>datetime</th>\n",
       "      <th>text</th>\n",
       "      <th>rating_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>г. Москва</td>\n",
       "      <td>False</td>\n",
       "      <td>Жалоба</td>\n",
       "      <td>0</td>\n",
       "      <td>лицензия № 2562</td>\n",
       "      <td>uhnov1</td>\n",
       "      <td>Бинбанк</td>\n",
       "      <td>2015-06-08 12:50:54</td>\n",
       "      <td>Добрый день! Я не являюсь клиентом банка и пор...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>г. Новосибирск</td>\n",
       "      <td>False</td>\n",
       "      <td>Не могу пользоваться услугой Сбербанк он-лайн</td>\n",
       "      <td>0</td>\n",
       "      <td>лицензия № 1481</td>\n",
       "      <td>Foryou</td>\n",
       "      <td>Сбербанк России</td>\n",
       "      <td>2015-06-08 11:09:57</td>\n",
       "      <td>Доброго дня! Являюсь держателем зарплатной кар...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>г. Москва</td>\n",
       "      <td>False</td>\n",
       "      <td>Двойное списание за один товар.</td>\n",
       "      <td>1</td>\n",
       "      <td>лицензия № 2562</td>\n",
       "      <td>Vladimir84</td>\n",
       "      <td>Бинбанк</td>\n",
       "      <td>2015-06-05 20:14:28</td>\n",
       "      <td>Здравствуйте!  Дублирую свое заявление от 03.0...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>г. Ставрополь</td>\n",
       "      <td>False</td>\n",
       "      <td>Меняют проценты комиссии  не предупредив и не ...</td>\n",
       "      <td>2</td>\n",
       "      <td>лицензия № 1481</td>\n",
       "      <td>643609</td>\n",
       "      <td>Сбербанк России</td>\n",
       "      <td>2015-06-05 13:51:01</td>\n",
       "      <td>Добрый день!! Я открыл расчетный счет в СберБа...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>г. Челябинск</td>\n",
       "      <td>False</td>\n",
       "      <td>Верните денежные средства за страховку</td>\n",
       "      <td>1</td>\n",
       "      <td>лицензия № 2766</td>\n",
       "      <td>anfisa-2003</td>\n",
       "      <td>ОТП Банк</td>\n",
       "      <td>2015-06-05 10:58:12</td>\n",
       "      <td>04.03.2015 г. взяла кредит в вашем банке, заяв...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             city  rating_not_checked  \\\n",
       "0       г. Москва               False   \n",
       "1  г. Новосибирск               False   \n",
       "2       г. Москва               False   \n",
       "3   г. Ставрополь               False   \n",
       "4    г. Челябинск               False   \n",
       "\n",
       "                                               title  num_comments  \\\n",
       "0                                             Жалоба             0   \n",
       "1      Не могу пользоваться услугой Сбербанк он-лайн             0   \n",
       "2                    Двойное списание за один товар.             1   \n",
       "3  Меняют проценты комиссии  не предупредив и не ...             2   \n",
       "4             Верните денежные средства за страховку             1   \n",
       "\n",
       "      bank_license       author        bank_name             datetime  \\\n",
       "0  лицензия № 2562       uhnov1          Бинбанк  2015-06-08 12:50:54   \n",
       "1  лицензия № 1481       Foryou  Сбербанк России  2015-06-08 11:09:57   \n",
       "2  лицензия № 2562   Vladimir84          Бинбанк  2015-06-05 20:14:28   \n",
       "3  лицензия № 1481       643609  Сбербанк России  2015-06-05 13:51:01   \n",
       "4  лицензия № 2766  anfisa-2003         ОТП Банк  2015-06-05 10:58:12   \n",
       "\n",
       "                                                text  rating_grade  \n",
       "0  Добрый день! Я не являюсь клиентом банка и пор...           NaN  \n",
       "1  Доброго дня! Являюсь держателем зарплатной кар...           NaN  \n",
       "2  Здравствуйте!  Дублирую свое заявление от 03.0...           NaN  \n",
       "3  Добрый день!! Я открыл расчетный счет в СберБа...           NaN  \n",
       "4  04.03.2015 г. взяла кредит в вашем банке, заяв...           NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()\n",
    "regex = re.compile(\"[А-Яа-я]+\")\n",
    "\n",
    "def words_only(text, regex=regex):\n",
    "    try:\n",
    "        return \" \".join(regex.findall(text))\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def lemmatize(text, mystem=m):\n",
    "    try:\n",
    "        return \"\".join(m.lemmatize(text)).strip()  \n",
    "    except:\n",
    "        return \" \"\n",
    "\n",
    "def stemming(text, stemmer = RussianStemmer()):\n",
    "    try:\n",
    "        return \" \".join([stemmer.stem(w) for w in text.split()])\n",
    "    except:\n",
    "        return \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Очистка\n",
    "- оставляются только слова\n",
    "- лемметизация\n",
    "- стемминг\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vvs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/vvs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 36s, sys: 7.1 s, total: 19min 43s\n",
      "Wall time: 37min 32s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vvs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = train[(train['rating_grade'] == 5)|(train['rating_grade'] == 1)]\n",
    "\n",
    "data['text_words'] = data.apply(lambda row: words_only(row['text']), axis = 1)\n",
    "data['text_lemmas'] = data.apply(lambda row: lemmatize(row['text_words']), axis = 1)\n",
    "data['text_stemm'] = data.apply(lambda row: stemming(row['text_lemmas']), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример\n",
    "- До очистки \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Открыт вклад и счет в USD. Плюс к этому есть зарплатная карта, в рублях, само собой. Сегодня пришел в указанное отделение с целью пополнить долларовый счёт на 700 USD.\\xa0Дал операционисту паспорт, зарплатную карту (т.к. на окошке написано \"приготовьте карту для подтверждения операции\" или что-то подобное и в прошлый раз у меня ее потребовали) и сказал, что нужно положить деньги на ДОЛЛАРОВЫЙ счет.\\xa0Операционист всё взяла, что-то делала-крутила-вертела, вставила карту в терминал, сказала \"введите пин\", я ввёл пин, получил в ответ чек, где было написано, что доллары были внесены.... на счёт КАРТЫ! в РУБЛЯХ! Вопрос банку №1, риторический:  Я не понимаю, кем нужно быть, чтобы сознательно проводить такие операции??? за углом, меньше чем через квартал, курс приёма валюты выше почти на рубль! Если я действительно хотел совершить такую \"хитрую\" операцию, мне было выгоднее сделать 100 шагов и \"заработать\" на этом около 700 рублей, после чего просто внести рубли на счёт карты в банкомате! Далее...\\xa0 После долгих, около двух часов, выяснений отношений с операционистом и\\xa0(видимо)\\xa0ее начальником, их попыток отменить первую транзакцию и т.д. и т.п., мы \"сошлись\" на том, что мне компенсируют курсовую разницу между зачислением долларов на рублёвую карту (54,6 руб/доллар, со СЛОВ операциониста) и курсом покупки валюты (58,7 руб/доллар), я куплю за наличные те же 700 долларов и положу их на долларовый счёт. Также я попросил указать на чеке курс, по которому прошла первая транзакция (доллары на рублёвый счёт, 54,6) и поставить печать.\\xa0 Доллары поступили на счёт через 3 часа и по курсу (тадааам!) 54,38. Комиссия банка? или что это было?. Опять же, операционист не знает о комиссиях или курсах проведения операции? Вывод 1:  В итоге я потерял 2 часа времени, нервы и (фиг уж с ними) 154 рубля. Вывод 2:  другим клиентам: требуйте распечатки тех операций, которые собирается провести операционист ДО их проведения. Если бы не было \"высокотехнологичного\" подтверждения операции PIN\\'ом, а была обычная платёжка, всего этого можно было бы избежать. Вопрос банку №2, риторический:  \\xa0Сбер, когда вы будете брать на работу компетентных сотрудников? Которые способны думать или, хотя бы, слушать, что их просят сделать?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data['text'][19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример\n",
    "- после очистки\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'открыва вклад и счет в плюс к эт быт зарплатн карт в рубл сам себ сегодн приход в указа отделен с цел пополня долларов сч т на дава операционист паспорт зарплатн карт т к на окошк написа приготавлива карт для подтвержден операц ил что то подобн и в прошл раз у я он потребова и сказа что нужн полага деньг на долларов счет операционист вс взят что то дела крут вертел вставля карт в термина сказа ввод пин я вв л пин получа в ответ чек где быт написа что доллар быт внос на сч т карт в рубл вопрос банк риторическ я не понима кто нужн быт чтоб сознательн провожа так операц за угол мал чем через кварта курс при ма валют высок почт на рубл есл я действительн хотет соверша так хитр операц я быт выгодн сдела шаг и зарабатыва на эт окол рубл посл что прост внос рубл на сч т карт в банкомат дал посл долг окол два час выяснен отношен с операционист и видим е начальник их попытк отменя перв транзакц и т д и т п мы сход на то что я компенсирова курсов разниц межд зачислен доллар на рубл ву карт руб доллар со слов операционист и курс покупк валют руб доллар я куп за наличн тот же доллар и полага он на долларов сч т такж я попрос указыва на чек курс по котор проход перв транзакц доллар на рубл вы сч т и поставля печа доллар поступа на сч т через час и по курс тадаа комисс банк ил что эт быт опя же операционист не знат о комисс ил курс проведен операц вывод в итог я потеря час врем нерв и фиг уж с он рубл вывод друг клиент требова распечатк тот операц котор собира провод операционист до их проведен есл бы не быт высокотехнологичн подтвержден операц ом а быт обычн плат жка ве эт можн быт бы избега вопрос банк риторическ сбер когд вы быт брат на работ компетентн сотрудник котор способн дума ил хот бы слуша что он прос сдела'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data.head()\n",
    "data['text_stemm'][19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### разделение на тестовую и обучающую части\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data['text_stemm'], data['rating_grade'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### создание словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 24s, sys: 2.63 s, total: 1min 27s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "texts = [X_train.iloc[i].split() for i in range(len(X_train))]\n",
    "dictionary = Dictionary(texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проба последовательной обработки  и классификации\n",
    " -  TfidfVectorizer\n",
    " -  Lsi (TruncatedSVD)\n",
    " -  LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 294 µs, sys: 4 µs, total: 298 µs\n",
      "Wall time: 305 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clf = LogisticRegression(penalty='l2', C=0.1)\n",
    "\n",
    "pipe = Pipeline([('tfidf', TfidfVectorizer()), ('tm', TruncatedSVD()), ('classifier', clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.4 s, sys: 388 ms, total: 16.8 s\n",
      "Wall time: 16.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "score = pipe.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.764846532962475"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Проба улучшить качество работы заменой классификатора на GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 206 µs, sys: 2 µs, total: 208 µs\n",
      "Wall time: 217 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()), \n",
    "    ('tm', TruncatedSVD()), \n",
    "    ('classifier', clf)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.1 s, sys: 248 ms, total: 20.3 s\n",
      "Wall time: 19.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "score = pipe.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7837798272580881"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метрика стала немного лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_russian = [\"и\", \"в\", \"во\",  \"что\", \"он\", \"на\", \"я\", \"с\", \"со\", \"как\", \"а\", \"то\", \"все\", \"она\", \"так\", \n",
    "                      \"его\", \"но\", \"да\", \"ты\", \"к\", \"у\", \"же\", \"вы\", \"за\", \"бы\", \"по\", \"только\", \"ее\", \"мне\", \"было\", \n",
    "                      \"вот\", \"от\", \"меня\", \"еще\", \"о\", \"из\", \"ему\", \"теперь\", \"когда\", \"даже\", \"ну\", \"вдруг\", \n",
    "                      \"ли\", \"если\", \"уже\", \"или\", \"ни\", \"быть\", \"был\", \"него\", \"до\", \"вас\", \"нибудь\", \"опять\", \"уж\", \n",
    "                      \"вам\", \"ведь\", \"там\", \"потом\", \"себя\", \"ничего\", \"ей\", \"может\", \"они\", \"тут\", \"где\", \"есть\", \"надо\", \n",
    "                      \"ней\", \"для\", \"мы\", \"тебя\", \"их\", \"чем\", \"была\", \"сам\", \"чтоб\", \"без\", \"будто\", \"чего\", \"раз\", \n",
    "                      \"тоже\", \"себе\", \"под\", \"будет\", \"ж\", \"тогда\", \"кто\", \"этот\", \"того\", \"потому\", \"этого\", \"какой\", \n",
    "                      \"совсем\", \"ним\", \"здесь\", \"этом\", \"один\", \"почти\", \"мой\", \"тем\", \"чтобы\", \"нее\", \"сейчас\", \"были\", \n",
    "                      \"куда\", \"зачем\", \"всех\", \"никогда\", \"можно\", \"при\", \"наконец\", \"два\", \"об\", \"другой\", \"хоть\", \"после\", \n",
    "                      \"над\", \"больше\", \"тот\", \"через\", \"эти\", \"нас\", \"про\", \"всего\", \"них\", \"какая\", \"много\", \"разве\", \"три\", \n",
    "                      \"эту\", \"моя\", \"впрочем\", \"хорошо\", \"свою\", \"этой\", \"перед\", \"иногда\", \"лучше\", \"чуть\", \"том\", \"нельзя\", \n",
    "                      \"такой\", \"им\", \"более\", \"всегда\", \"конечно\", \"всю\", \"между\", \"при\", \"однако\", \"это\", \"всё\", \"весь\",\n",
    "                      \"обо\", \"ваш\" , \" \" , \"  \", \" - \", \"-\", \"   \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_param_grid = {\n",
    "    \"tm\": [\"passthrough\", TruncatedSVD(5),TruncatedSVD(10),TruncatedSVD(15), TruncatedSVD(20),TruncatedSVD(25)],\n",
    "    \"tfidf__analyzer\": [\"word\", \"char\"],\n",
    "    \"tfidf__smooth_idf\": [True, False],\n",
    "    \"tfidf__ngram_range\": [(1, 1), (1, 2)],\n",
    "    \"tfidf__use_idf\": [True, False],\n",
    "    \"tfidf__stop_words\": [None, stop_words_russian],\n",
    "    \"classifier__loss\":[\"deviance\"],\n",
    "    \"classifier__learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "    \"classifier__min_samples_split\": np.linspace(0.1, 0.5, 12),\n",
    "    \"classifier__min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n",
    "    \"classifier__max_depth\":[3,5,8],\n",
    "    \"classifier__max_features\":[\"log2\",\"sqrt\"],\n",
    "    \"classifier__criterion\": [\"friedman_mse\",  \"mae\"],\n",
    "    \"classifier__subsample\":[0.5, 0.8, 0.9, 1.0],\n",
    "    \"classifier__n_estimators\":[10]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Поиск оптимальных параметров алгоритма обработки с помощью RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 20.2min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed: 24.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed: 24.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.1 s, sys: 10.4 s, total: 1min 9s\n",
      "Wall time: 25min 25s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score=nan,\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('tfidf',\n",
       "                                              TfidfVectorizer(analyzer='word',\n",
       "                                                              binary=False,\n",
       "                                                              decode_error='strict',\n",
       "                                                              dtype=<class 'numpy.float64'>,\n",
       "                                                              encoding='utf-8',\n",
       "                                                              input='content',\n",
       "                                                              lowercase=True,\n",
       "                                                              max_df=1.0,\n",
       "                                                              max_features=None,\n",
       "                                                              min_df=1,\n",
       "                                                              ngram_range=(1,\n",
       "                                                                           1),\n",
       "                                                              norm='l2',\n",
       "                                                              preprocessor=None,\n",
       "                                                              smooth_idf=True,\n",
       "                                                              stop_words=None,...\n",
       "                                               TruncatedSVD(algorithm='randomized',\n",
       "                                                            n_components=15,\n",
       "                                                            n_iter=5,\n",
       "                                                            random_state=None,\n",
       "                                                            tol=0.0),\n",
       "                                               TruncatedSVD(algorithm='randomized',\n",
       "                                                            n_components=20,\n",
       "                                                            n_iter=5,\n",
       "                                                            random_state=None,\n",
       "                                                            tol=0.0),\n",
       "                                               TruncatedSVD(algorithm='randomized',\n",
       "                                                            n_components=25,\n",
       "                                                            n_iter=5,\n",
       "                                                            random_state=None,\n",
       "                                                            tol=0.0)]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=8)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "search = RandomizedSearchCV(pipe, param_distributions=boost_param_grid, verbose=8,n_jobs = -1)\n",
    "search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.7634802127555751\n",
      "f1_score 0.8656782596646806\n"
     ]
    }
   ],
   "source": [
    "print(f'acc {accuracy_score(y_test, y_pred)}')\n",
    "print(f'f1_score {f1_score(y_test, y_pred)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Итоговые покзатели немного хуже. Скорее связано с тем, что было найдено более устойчивое решение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проба последовательной обработки  и классификации\n",
    " -  TfidfVectorizer\n",
    " -  LDA\n",
    " -  SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 384 µs, sys: 10 µs, total: 394 µs\n",
      "Wall time: 1.61 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "clf = SVC()\n",
    "\n",
    "pipe2 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()), \n",
    "    ('tm', LDA()), \n",
    "    ('classifier', clf)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21 s, sys: 1.15 s, total: 22.2 s\n",
      "Wall time: 25.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "score = pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='...\n",
       "                                            learning_rate=0.1, loss='deviance',\n",
       "                                            max_depth=3, max_features=None,\n",
       "                                            max_leaf_nodes=None,\n",
       "                                            min_impurity_decrease=0.0,\n",
       "                                            min_impurity_split=None,\n",
       "                                            min_samples_leaf=1,\n",
       "                                            min_samples_split=2,\n",
       "                                            min_weight_fraction_leaf=0.0,\n",
       "                                            n_estimators=100,\n",
       "                                            n_iter_no_change=None,\n",
       "                                            presort='deprecated',\n",
       "                                            random_state=None, subsample=1.0,\n",
       "                                            tol=0.0001, validation_fraction=0.1,\n",
       "                                            verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_linear_svc = score.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.7835846386571024\n",
      "f1_score 0.871154237238895\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "print(f'acc {accuracy_score(y_test, y_pred_linear_svc)}')\n",
    "print(f'f1_score {f1_score(y_test, y_pred_linear_svc)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  LDA + SVC дали  результат немного лучшего качества\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio(url=\"https://wav-sounds.com/wp-content/uploads/2017/10/Herbert-06.wav\",autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Использованные материалы\n",
    "https://medium.com/swlh/randomized-or-grid-search-with-pipeline-cheatsheet-719c72eda68\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC\n",
    "\n",
    "https://www.kaggle.com/hatone/gradientboostingclassifier-with-gridsearchcv\n",
    "\n",
    "https://towardsdatascience.com/boosting-showdown-scikit-learn-vs-xgboost-vs-lightgbm-vs-catboost-in-sentiment-classification-f7c7f46fd956"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
